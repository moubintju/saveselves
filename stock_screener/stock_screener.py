import pandas as pd
from datetime import datetime, timedelta
import logging
import time
from data_fetcher import StockDataFetcher

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StockScreener:
    def __init__(self):
        self.data_fetcher = StockDataFetcher()
        self.screening_results = []
        self.processed_stocks_count = 0
        self.screening_start_time = None
        self.screening_end_time = None
        
    def screen_rescue_stocks(self, target_date=None, progress_callback=None, max_stocks=100):
        """ç­›é€‰å¯ä»¥è‡ªæ•‘çš„è‚¡ç¥¨"""
        if target_date is None:
            target_date = datetime.now().strftime("%Y-%m-%d")
            
        logger.info(f"å¼€å§‹ç­›é€‰ {target_date} çš„è‡ªæ•‘è‚¡ç¥¨...")
        
        # è·å–æ‰€æœ‰è‚¡ç¥¨
        all_stocks = self.data_fetcher.get_all_stocks()
        if all_stocks is None or len(all_stocks) == 0:
            logger.error("æ— æ³•è·å–è‚¡ç¥¨æ•°æ®")
            return []
            
        total_stocks = min(len(all_stocks), max_stocks)
        logger.info(f"å…±éœ€è¦ç­›é€‰ {total_stocks} åªè‚¡ç¥¨ (é™åˆ¶ä¸ºå‰{max_stocks}åª)")
        
        rescue_stocks = []
        processed_count = 0
        
        # é™åˆ¶å¤„ç†çš„è‚¡ç¥¨æ•°é‡ä»¥é¿å…è¶…æ—¶
        limited_stocks = all_stocks.head(max_stocks)
        
        for index, stock in limited_stocks.iterrows():
            processed_count += 1
            stock_code = stock['ä»£ç ']
            stock_name = stock['åç§°']
            
            # æŠ¥å‘Šè¿›åº¦
            if progress_callback:
                progress = int((processed_count / total_stocks) * 100)
                progress_callback(progress, f"æ­£åœ¨åˆ†æ: {stock_name}({stock_code})")
            
            # æ‰§è¡Œç­›é€‰é€»è¾‘
            if self.check_rescue_criteria(stock, stock_code):
                rescue_stocks.append({
                    'code': stock_code,
                    'name': stock_name,
                    'current_price': stock.get('æœ€æ–°ä»·', 0),
                    'change_pct': stock.get('æ¶¨è·Œå¹…', 0),
                    'volume': stock.get('æˆäº¤é‡', 0),
                    'turnover': stock.get('æˆäº¤é¢', 0),
                    'market_cap': stock.get('æ€»å¸‚å€¼', 0)
                })
            
            # æ¯å¤„ç†5åªè‚¡ç¥¨å¢åŠ å°å»¶è¿Ÿï¼Œé™ä½è¯·æ±‚é¢‘ç‡
            if processed_count % 5 == 0:
                time.sleep(0.1)  # 100mså»¶è¿Ÿ
                
            # æ¯å¤„ç†10åªè‚¡ç¥¨è®°å½•ä¸€æ¬¡è¿›åº¦
            if processed_count % 10 == 0:
                logger.info(f"å·²å¤„ç† {processed_count}/{total_stocks} åªè‚¡ç¥¨ï¼Œæ‰¾åˆ° {len(rescue_stocks)} åªç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨")
        
        logger.info(f"ç­›é€‰å®Œæˆï¼å…±æ‰¾åˆ° {len(rescue_stocks)} åªç¬¦åˆè‡ªæ•‘æ¡ä»¶çš„è‚¡ç¥¨")
        self.screening_results = rescue_stocks
        return rescue_stocks
    
    def screen_rescue_stocks_batch(self, target_date=None, batch_start=0, batch_size=20):
        """åˆ†æ‰¹ç­›é€‰å¯ä»¥è‡ªæ•‘çš„è‚¡ç¥¨"""
        if target_date is None:
            target_date = datetime.now().strftime("%Y-%m-%d")
        
        # è®°å½•ç­›é€‰å¼€å§‹æ—¶é—´
        if batch_start == 0:
            self.screening_start_time = datetime.now()
            
        logger.info(f"ğŸš€ å¼€å§‹åˆ†æ‰¹ç­›é€‰ {target_date} çš„è‡ªæ•‘è‚¡ç¥¨ï¼Œæ‰¹æ¬¡ {batch_start}-{batch_start + batch_size}...")
        logger.info(f"ğŸ“Š å½“å‰APIç»Ÿè®¡: {self.data_fetcher.get_api_statistics()}")
        
        # è·å–æ‰€æœ‰è‚¡ç¥¨
        all_stocks = self.data_fetcher.get_all_stocks()
        if all_stocks is None or len(all_stocks) == 0:
            logger.error("æ— æ³•è·å–è‚¡ç¥¨æ•°æ®")
            return {
                'results': [],
                'total_stocks': 0,
                'processed_count': 0,
                'has_more': False
            }
            
        total_stocks = len(all_stocks)
        logger.info(f"æ€»å…±æœ‰ {total_stocks} åªè‚¡ç¥¨éœ€è¦ç­›é€‰")
        
        # è·å–å½“å‰æ‰¹æ¬¡çš„è‚¡ç¥¨
        batch_end = min(batch_start + batch_size, total_stocks)
        batch_stocks = all_stocks.iloc[batch_start:batch_end]
        
        logger.info(f"å½“å‰æ‰¹æ¬¡å¤„ç† {len(batch_stocks)} åªè‚¡ç¥¨ ({batch_start}-{batch_end})")
        
        rescue_stocks = []
        processed_count = batch_start
        
        for index, stock in batch_stocks.iterrows():
            processed_count += 1
            stock_code = stock['ä»£ç ']
            stock_name = stock['åç§°']
            
            logger.info(f"æ­£åœ¨åˆ†æ: {stock_name}({stock_code}) - {processed_count}/{total_stocks}")
            
            # æ‰§è¡Œç­›é€‰é€»è¾‘
            self.processed_stocks_count += 1
            if self.check_rescue_criteria(stock, stock_code):
                rescue_stocks.append({
                    'code': stock_code,
                    'name': stock_name,
                    'current_price': stock.get('æœ€æ–°ä»·', 0),
                    'change_pct': stock.get('æ¶¨è·Œå¹…', 0),
                    'volume': stock.get('æˆäº¤é‡', 0),
                    'turnover': stock.get('æˆäº¤é¢', 0),
                    'market_cap': stock.get('æ€»å¸‚å€¼', 0)
                })
            
            # æ¯å¤„ç†2åªè‚¡ç¥¨å¢åŠ å»¶è¿Ÿï¼Œé™ä½è¯·æ±‚é¢‘ç‡
            if (processed_count - batch_start) % 2 == 0:
                time.sleep(0.2)  # 200mså»¶è¿Ÿ
        
        has_more = batch_end < total_stocks
        
        # è®°å½•ç­›é€‰ç»“æŸæ—¶é—´
        if not has_more:
            self.screening_end_time = datetime.now()
        
        # è·å–APIç»Ÿè®¡ä¿¡æ¯
        api_stats = self.data_fetcher.get_api_statistics()
        
        logger.info(f"âœ… æ‰¹æ¬¡ç­›é€‰å®Œæˆï¼æœ¬æ‰¹æ¬¡æ‰¾åˆ° {len(rescue_stocks)} åªç¬¦åˆè‡ªæ•‘æ¡ä»¶çš„è‚¡ç¥¨")
        logger.info(f"ğŸ“Š APIè°ƒç”¨ç»Ÿè®¡: æ€»è®¡{api_stats['total_calls']}æ¬¡ï¼ŒæˆåŠŸ{api_stats['successful_calls']}æ¬¡ï¼Œå¤±è´¥{api_stats['failed_calls']}æ¬¡")
        
        return {
            'results': rescue_stocks,
            'total_stocks': total_stocks,
            'processed_count': processed_count,
            'has_more': has_more,
            'api_calls_made': api_stats['total_calls'],
            'api_success_rate': api_stats['success_rate'],
            'verification_info': {
                'data_source': 'akshare',
                'real_data_confirmed': api_stats['data_source_verified'],
                'processing_timestamp': datetime.now().isoformat(),
                'api_statistics': api_stats
            }
        }
    
    def check_rescue_criteria(self, stock_data, stock_code):
        """æ£€æŸ¥è‚¡ç¥¨æ˜¯å¦ç¬¦åˆè‡ªæ•‘æ ‡å‡†"""
        try:
            # è·å–å†å²æ•°æ®ï¼ˆAPIè°ƒç”¨ç»Ÿè®¡å·²åœ¨data_fetcherä¸­å¤„ç†ï¼‰
            hist_data = self.data_fetcher.get_stock_history(stock_code, days=5)
            if hist_data is None or len(hist_data) < 2:
                return False
                
            # æœ€æ–°äº¤æ˜“æ—¥æ•°æ®
            today_data = hist_data.iloc[-1]
            yesterday_data = hist_data.iloc[-2] if len(hist_data) > 1 else None
            
            # æå–ä»·æ ¼æ•°æ®
            today_open = today_data.get('å¼€ç›˜', 0)
            today_close = today_data.get('æ”¶ç›˜', 0)
            today_high = today_data.get('æœ€é«˜', 0)
            today_low = today_data.get('æœ€ä½', 0)
            today_volume = today_data.get('æˆäº¤é‡', 0)
            
            if yesterday_data is not None:
                yesterday_open = yesterday_data.get('å¼€ç›˜', 0)
                yesterday_close = yesterday_data.get('æ”¶ç›˜', 0)
                yesterday_volume = yesterday_data.get('æˆäº¤é‡', 0)
            else:
                return False
            
            # æ¡ä»¶1: å½“å¤©éæ¶¨åœ
            if self.data_fetcher.is_limit_up(today_open, today_close, stock_code):
                return False
                
            # æ¡ä»¶2: å½“å¤©ä¸ºå°é˜³çº¿
            if not self.data_fetcher.is_small_positive_line(today_open, today_close, today_high, today_low):
                return False
                
            # æ¡ä»¶3: å½“å¤©æˆäº¤é‡å°äºæ˜¨æ—¥æˆäº¤é‡
            if today_volume >= yesterday_volume:
                return False
                
            # æ¡ä»¶4: æ˜¨æ—¥éè·Œåœï¼Œæ˜¨æ—¥éæ¶¨åœ
            if (self.data_fetcher.is_limit_up(yesterday_open, yesterday_close, stock_code) or 
                self.data_fetcher.is_limit_down(yesterday_open, yesterday_close, stock_code)):
                return False
                
            # æ¡ä»¶5: è¿‘3æ—¥å†…é¦–æ¬¡æ¶¨åœï¼ˆé¦–æ¿ï¼‰- éœ€è¦æ›´å¤šå†å²æ•°æ®
            extended_hist = self.data_fetcher.get_stock_history(stock_code, days=10)
            if not self.data_fetcher.check_first_limit_up_in_3_days(extended_hist, stock_code):
                return False
                
            # æ¡ä»¶6: ä¸»æ¿è‚¡ç¥¨ - å·²åœ¨data_fetcherä¸­è¿‡æ»¤
            # æ¡ä»¶7: éSTè‚¡ç¥¨ - å·²åœ¨data_fetcherä¸­è¿‡æ»¤
            
            return True
            
        except Exception as e:
            logger.warning(f"æ£€æŸ¥è‚¡ç¥¨ {stock_code} å¤±è´¥: {e}")
            return False
    
    def get_screening_summary(self):
        """è·å–ç­›é€‰ç»“æœæ‘˜è¦"""
        if not self.screening_results:
            return {
                'total_count': 0,
                'avg_change_pct': 0,
                'avg_volume': 0,
                'total_market_cap': 0
            }
            
        results_df = pd.DataFrame(self.screening_results)
        
        return {
            'total_count': len(self.screening_results),
            'avg_change_pct': results_df['change_pct'].mean(),
            'avg_volume': results_df['volume'].mean(),
            'total_market_cap': results_df['market_cap'].sum(),
            'max_change_pct': results_df['change_pct'].max(),
            'min_change_pct': results_df['change_pct'].min()
        }
    
    def get_detailed_statistics(self):
        """è·å–è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯"""
        api_stats = self.data_fetcher.get_api_statistics()
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_time = None
        if self.screening_start_time and self.screening_end_time:
            processing_time = (self.screening_end_time - self.screening_start_time).total_seconds()
        
        return {
            'screening_statistics': {
                'total_processed': self.processed_stocks_count,
                'results_found': len(self.screening_results),
                'success_rate': (len(self.screening_results) / self.processed_stocks_count * 100) if self.processed_stocks_count > 0 else 0,
                'processing_time_seconds': processing_time,
                'start_time': self.screening_start_time.isoformat() if self.screening_start_time else None,
                'end_time': self.screening_end_time.isoformat() if self.screening_end_time else None
            },
            'api_statistics': api_stats,
            'data_verification': {
                'data_source': 'akshare',
                'real_data_confirmed': api_stats['data_source_verified'],
                'api_calls_per_stock': api_stats['total_calls'] / self.processed_stocks_count if self.processed_stocks_count > 0 else 0
            }
        }
    
    def export_results_to_excel(self, filename=None):
        """å¯¼å‡ºç»“æœåˆ°Excelæ–‡ä»¶"""
        if not self.screening_results:
            logger.warning("æ²¡æœ‰ç­›é€‰ç»“æœå¯å¯¼å‡º")
            return None
            
        if filename is None:
            filename = f"rescue_stocks_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
            
        try:
            df = pd.DataFrame(self.screening_results)
            
            # æ·»åŠ ä¸­æ–‡åˆ—å
            df.columns = ['è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨åç§°', 'æœ€æ–°ä»·', 'æ¶¨è·Œå¹…(%)', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æ€»å¸‚å€¼']
            
            # æ ¼å¼åŒ–æ•°æ®
            df['æ¶¨è·Œå¹…(%)'] = df['æ¶¨è·Œå¹…(%)'].round(2)
            df['æœ€æ–°ä»·'] = df['æœ€æ–°ä»·'].round(2)
            
            # ä¿å­˜åˆ°resultsç›®å½•
            filepath = f"results/{filename}"
            df.to_excel(filepath, index=False, engine='openpyxl')
            
            logger.info(f"ç»“æœå·²å¯¼å‡ºåˆ°: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºExcelå¤±è´¥: {e}")
            return None
    
    def export_results_to_csv(self, filename=None):
        """å¯¼å‡ºç»“æœåˆ°CSVæ–‡ä»¶"""
        if not self.screening_results:
            logger.warning("æ²¡æœ‰ç­›é€‰ç»“æœå¯å¯¼å‡º")
            return None
            
        if filename is None:
            filename = f"rescue_stocks_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
            
        try:
            df = pd.DataFrame(self.screening_results)
            
            # æ·»åŠ ä¸­æ–‡åˆ—å
            df.columns = ['è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨åç§°', 'æœ€æ–°ä»·', 'æ¶¨è·Œå¹…(%)', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æ€»å¸‚å€¼']
            
            # æ ¼å¼åŒ–æ•°æ®
            df['æ¶¨è·Œå¹…(%)'] = df['æ¶¨è·Œå¹…(%)'].round(2)
            df['æœ€æ–°ä»·'] = df['æœ€æ–°ä»·'].round(2)
            
            # ä¿å­˜åˆ°resultsç›®å½•
            filepath = f"results/{filename}"
            df.to_csv(filepath, index=False, encoding='utf-8-sig')
            
            logger.info(f"ç»“æœå·²å¯¼å‡ºåˆ°: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºCSVå¤±è´¥: {e}")
            return None